from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import logging
import time


logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def extract_text_safely(element, selector, get_attribute=None):
    """Safely extract text from an element with error handling."""
    try:
        if not element:
            return ""
        found = element.find_element(By.CSS_SELECTOR, selector)
        if not found:
            return ""
        if get_attribute:
            return found.get_attribute(get_attribute) or ""
        return found.text.strip()
    except:
        return ""

def find_element_safely(driver, selectors, wait_time=10):
    """Find an element with multiple selector attempts and error handling."""
    wait = WebDriverWait(driver, wait_time)
    for selector in selectors:
        try:
            element = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, selector)))
            if element and element.is_displayed():
                return element
        except:
            continue
    return None

def find_elements_safely(driver, selectors):
    """Find elements with multiple selector attempts and error handling."""
    for selector in selectors:
        try:
            elements = driver.find_elements(By.CSS_SELECTOR, selector)
            if elements:
                return elements
        except:
            continue
    return []

def scrape_g2(company_name):
    """
    Scrape reviews from G2 for the given company.
    Returns a list of review dictionaries.
    """
    reviews = []
    driver = None

    try:
        
        options = Options()
        options.add_argument("--headless")
        options.add_argument("--disable-gpu")
        options.add_argument("--window-size=1920,1080")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-blink-features=AutomationControlled")
        options.add_argument("--ignore-certificate-errors")
        options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36')
        options.add_argument('--accept-language=en-US,en;q=0.9')
        
        # Map common company names to their G2 slugs
        company_mapping = {
            "zoom-workplace": "zoom-workplace",
            "zoom": "zoom-video-conferencing",
            "zoom-video": "zoom-video-conferencing"
        }
        
        # Get the correct slug and construct URL
        actual_slug = company_mapping.get(company_name.lower(), company_name.lower().replace(' ', '-'))
        base_url = f"https://www.g2.com/products/{actual_slug}/reviews"
        logger.info(f"Starting scrape of G2 for company: {company_name} (using slug: {actual_slug})")
        
        # Initialize browser
        driver = webdriver.Chrome(options=options)
        wait = WebDriverWait(driver, 10)
        page = 1
        
        while True:
            try:
                # Fetch page
                url = f"{base_url}?page={page}"
                logger.info(f"Fetching page {page}")
                driver.get(url)
                time.sleep(5)  # Wait longer for dynamic content to load
                
                # Sometimes we need to scroll down to load all reviews
                driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(2)  # Wait for any lazy-loaded content
                
                
                if "Page Not Found" in driver.title:
                    logger.error(f"Product page not found. Please check if '{actual_slug}' is the correct product slug")
                    break
                    
                no_results = find_element_safely(driver, [
                    ".no-results",
                    ".empty-state",
                    ".error-state"
                ])
                if no_results:
                    logger.warning("No reviews found on G2")
                    break
                
                # Find review elements
                review_elements = find_elements_safely(driver, [
                    "div.paper",
                    "div.review",
                    "div[itemprop='review']",
                    "div.enterprise-review",
                    "div.v2__EIReviewsEIReviewCard__reviewCard",  # New G2 layout
                    "div[data-testid='review-card']",  # Another G2 layout variant
                    "article.review",  # Yet another possibility
                    "div.review-container"  # And another
                ])
                
                if not review_elements:
                    logger.info("No more reviews found")
                    break
                
                logger.info(f"Found {len(review_elements)} reviews on page {page}")
                
                # Process each review
                for elem in review_elements:
                    try:
                        # Extract title
                        title = extract_text_safely(elem, "h3") or \
                               extract_text_safely(elem, "div.review-title") or \
                               extract_text_safely(elem, "div.headline") or \
                               "No title"
                        
                        # Extract description
                        description = extract_text_safely(elem, "p.review-body") or \
                                    extract_text_safely(elem, "p.content") or \
                                    extract_text_safely(elem, "div.review-content p") or \
                                    extract_text_safely(elem, "div[itemprop='reviewBody']")
                        
                        # Extract date
                        date = extract_text_safely(elem, "time", "datetime") or \
                              extract_text_safely(elem, "div.review-date") or \
                              extract_text_safely(elem, "meta[itemprop='datePublished']", "content")
                        
                        # Extract rating
                        rating = ""
                        rating_element = find_element_safely(elem, [
                            "div.rating[aria-label]",
                            "meta[itemprop='ratingValue']",
                            "div.stars"
                        ])
                        if rating_element:
                            rating = rating_element.get_attribute("aria-label") or \
                                    rating_element.get_attribute("content")
                            if not rating:
                                stars = elem.find_elements(By.CSS_SELECTOR, "i.filled")
                                rating = str(len(stars))
                        
                        # Extract reviewer name
                        reviewer = extract_text_safely(elem, "span.user-name") or \
                                 extract_text_safely(elem, "div.reviewer-name") or \
                                 extract_text_safely(elem, "span[itemprop='author']") or \
                                 "Anonymous"
                        
                        # Add review if we have at least title or description
                        if title or description:
                            reviews.append({
                                "title": title,
                                "description": description,
                                "date": date,
                                "rating": rating,
                                "reviewer": reviewer
                            })
                            
                    except Exception as e:
                        logger.error(f"Error processing review: {str(e)}")
                        continue
                
                # Try to go to next page
                next_button = find_element_safely(driver, [
                    "a.next", 
                    "button.next",
                    "a[aria-label='Next']",
                    "button[aria-label='Next']",
                    "a:contains('Next')",
                    "button:contains('Next')"
                ])
                
                if not next_button or not next_button.is_enabled():
                    logger.info("Reached last page")
                    break
                
                # Click next page button
                try:
                    driver.execute_script("arguments[0].scrollIntoView(true);", next_button)
                    time.sleep(1)
                    next_button.click()
                except:
                    driver.execute_script("arguments[0].click();", next_button)
                
                page += 1
                time.sleep(3)  # Wait between pages
                
            except Exception as e:
                logger.error(f"Error on page {page}: {str(e)}")
                break

    except Exception as e:
        logger.error(f"Error during scraping: {str(e)}")
    
    finally:
        if driver:
            try:
                driver.quit()
            except:
                pass
    
    logger.info(f"Scraped {len(reviews)} reviews total")
    return reviews